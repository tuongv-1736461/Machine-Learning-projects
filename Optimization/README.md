
# Comparing Model Fits and Training Data for a Given Dataset

In the first section, the report identifies the best-fitting parameters for a model of form f(x) = Acos(Bx) + Cx + D and generates a 2D loss landscape. The second section fits three different models (line, parabola, and 19th-degree polynomial) to the data and computes the least square error for each model over training and test data. By comparing the results from different training data, this report highlights the importance of exploring various models and training data to achieve the best fit for a given dataset.






## Author

- [Jenny Van](https://github.com/tuongv-1736461)






## Section I: Introduction and Overview

The goal of the report is to explore various models and training data to achieve the best fit for a given dataset. The first part of the analysis involves finding the parameters A, B, C, and D of a model of the form f(x) = Acos(Bx) + Cx + D that best fits the data. Then, a 2D loss landscape is generated by sweeping through values of the two remaining parameters with two fixed parameters. The second part of the analysis involves fitting three different models (line, parabola, and 19th degree polynomial) to the first 20 data points as training data and computing the least square error for each model over the training points, as well as the remaining 10 data points as test data. Finally, the same analysis is repeated with the first 10 and last 10 data points as training data, and the results are compared with the previous analysis. The next section of the report, Section II, will cover the theoretical background of the analysis. Section III will detail the algorithm implementation and development, while Section IV will present the results. Finally, Section V will conclude with the best models and training data that fit the data, based on the analysis

## Section II: Theoretical Background 

The models used in this analysis are:

A sinusoidal function with a linear term and constant: 

    f(x) = Acos(Bx) + Cx + D

A line 

    f(x) = mx + b

A parabola  

    f(x) = ax^2 + bx + c

A 19th degree polynomial

    f(x) = a0 + a1x + a2x^2 + ... + a19x^19

The least squares error method is a widely used technique for finding a function that best fits a given set of data points. It works by minimizing the sum of the squared errors between the function and the data points. For a set of n data points (x1, y1), (x2, y2), ..., (xn, yn), the method seeks to find a function f(x) that minimizes the squared error for each data point (xi, yi), which is calculated as (f(xi) - yi)^2. The least squares error is obtained by taking the square root of the average of the squared errors, as given by the formula:

    E = sqrt((1/n) * sum((f(xi) - yi)^2))

To minimize this error, the parameters of the model are adjusted accordingly.


## Section III: Algorithm Implementation and Development 

## Part i: Determine the parameters A, B, C, D for the minimum error 
This code implements a non-linear regression model using the least squares method to fit a sinusoidal function with a linear term and constant to a dataset. The algorithm first defines the model function which takes in the input data x and model parameters A, B, C, and D, and returns the predicted output value based on the given function. The next step is to define the error function which calculates the difference between the predicted values and actual values and returns the root mean square error. This error function is then minimized using the Scipy optimization library's minimize function, which takes in the initial guess for the parameters, the error function, and the input and output data. Finally, the optimized values of the parameters A, B, C, and D are printed out along with the minimum error achieved by the model. This approach can be used to fit various types of models to a given dataset, and is particularly useful when dealing with non-linear functions that cannot be fit using linear regression models. 

 ```
import numpy as np
from scipy.optimize import minimize

# Define the model function
def model(x, params):
    A, B, C, D = params
    return A*np.cos(B*x) + C*x + D

# Define the least squares error function
def error(params, x, y):
    return np.sqrt(np.mean((model(x, params) - y)**2))

# Load data
X = np.arange(0,31)
Y = np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,
              40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])

# Part (i): Find the minimum error and determine the parameters A, B, C, D
initial_guess = [1, 1, 1, 1]
result = minimize(error, initial_guess, args=(X, Y))
params = result.x
print("Minimum error:", result.fun)
print("A:", params[0])
print("B:", params[1])
print("C:", params[2])
print("D:", params[3])
```   
## Part ii: 2D Loss Landscape 

In this implementation, we explore the 2D loss (error) landscape by sweeping through different combinations of parameter values. First, we define delta values for each parameter, which determine the range of values we sweep through. The delta value is chosen randomly and adjusted to visualize the minimum loss value best. Then, we use nested loops to sweep through all possible combinations of two fixed parameters and two swept parameters. For each combination, we calculate the corresponding loss using the error function defined earlier.

Then, we plot the resulting loss landscapes using the imshow function, which creates a grid of colored squares to represent the loss values. The cmap parameter specifies the color map to use, and the extent parameter sets the range of values for the x and y axes. The origin parameter specifies the position of the origin (lower left or upper left), and aspect parameter sets the aspect ratio of the plot. We also add axis labels, a title, and a color bar to each plot.

```
import matplotlib.pyplot as plt

# Part (ii): Sweep through values of two parameters and generate a 2D loss landscape
A = params[0]
B = params[1]
C = params[2]
D = params[3]

delta_a = 300
delta_b = 1000
delta_c = 500
delta_d = 60

# Sweep through values of A and B
loss_landscape_AB = np.zeros((100, 100))
for i, a in enumerate(np.linspace(A-delta_a, A+delta_a, 100)):
    for j, b in enumerate(np.linspace(B-delta_b, B+delta_b, 100)):
        params = [a, b, C, D]
        loss_landscape_AB[j, i] = error(params, X, Y)
# Generate a plot of the loss landscape
plt.imshow(loss_landscape_AB, cmap='viridis', extent=[A-delta_a, A+delta_a, B-delta_b, B+delta_b], origin='lower', aspect='auto')
plt.xlabel('A')
plt.ylabel('B')
plt.title('Loss Landscape')
plt.colorbar()
plt.show()

# Sweep through values of A and C
plt.figure() # create a new figure
loss_landscape_AC = np.zeros((100, 100))
for i, a in enumerate(np.linspace(A-delta_a, A+delta_a, 100)):
    for j, c in enumerate(np.linspace(C-delta_c, C+delta_c, 100)):
        params = [a, B, c, D]
        loss_landscape_AC[j, i] = error(params, X, Y)
# Generate a plot of the loss landscape
plt.imshow(loss_landscape_AC, cmap='viridis', extent=[A-delta_a, A+delta_a, C-delta_c, C+delta_c], origin='lower', aspect='auto')
plt.xlabel('A')
plt.ylabel('C')
plt.title('Loss Landscape')
plt.colorbar()
plt.show()


# Sweep through values of A and D
loss_landscape_AD = np.zeros((100, 100))
for i, a in enumerate(np.linspace(A-delta_a, A+delta_a, 100)):
    for j, d in enumerate(np.linspace(D-delta_d, D+delta_d, 100)):
        params = [a, B, C, d]
        loss_landscape_AD[j, i] = error(params, X, Y)
# Generate a plot of the loss landscape
plt.imshow(loss_landscape_AD, cmap='viridis', extent=[A-delta_a, A+delta_a, D-delta_d, D+delta_d], origin='lower', aspect='auto')
plt.xlabel('A')
plt.ylabel('D')
plt.title('Loss Landscape')
plt.colorbar()
plt.show()

# Sweep through values of B and C
loss_landscape_BC = np.zeros((100, 100))
for i, b in enumerate(np.linspace(B-delta_b, B+delta_b, 100)):
    for j, c in enumerate(np.linspace(C-delta_c, C+delta_c, 100)):
        params = [A, b, c, D]
        loss_landscape_BC[j, i] = error(params, X, Y)
# Generate a plot of the loss landscape
plt.imshow(loss_landscape_BC, cmap='viridis', extent=[B-delta_b, B+delta_b, C-delta_c, C+delta_c], origin='lower', aspect='auto')
plt.xlabel('B')
plt.ylabel('C')
plt.title('Loss Landscape')
plt.colorbar()
plt.show()

# Sweep through values of B and D
loss_landscape_BD = np.zeros((100, 100))
for i, b in enumerate(np.linspace(B-delta_b, B+delta_b, 100)):
    for j, d in enumerate(np.linspace(D-delta_d, D+delta_d, 100)):
        params = [A, b, C, d]
        loss_landscape_BD[j, i] = error(params, X, Y)
# Generate a plot of the loss landscape
plt.imshow(loss_landscape_BC, cmap='viridis', extent=[B-delta_b, B+delta_b, D-delta_d, D+delta_d], origin='lower', aspect='auto')
plt.xlabel('B')
plt.ylabel('D')
plt.title('Loss Landscape')
plt.colorbar()
plt.show()

# Sweep through values of C and D
loss_landscape_CD = np.zeros((100, 100))
for i, c in enumerate(np.linspace(C-delta_c, C+delta_c, 100)):
    for j, d in enumerate(np.linspace(D-delta_d, D+delta_d, 100)):
        params = [A, B, c, d]
        loss_landscape_CD[j, i] = error(params, X, Y)
# Generate a plot of the loss landscape
plt.imshow(loss_landscape_BC, cmap='viridis', extent=[C-delta_c, C+delta_c, D-delta_d, D+delta_d], origin='lower', aspect='auto')
plt.xlabel('C')
plt.ylabel('D')
plt.title('Loss Landscape')
plt.colorbar()
plt.show()
``` 
## Part iii: Fitting three different models  to the data and computes the least square error for each model over training and test data

The code is a solution to the problem of fitting a line, parabola, and 19th degree polynomial to a given set of data points, and computing the least square error for each of these models over the training points and the test data.

The first step is to split the given dataset into training and test sets, where the first 20 data points are used as training data and the remaining 10 points are used as test data. The X and Y arrays are reshaped to have a single feature column, as required by the LinearRegression model.

Next, a linear regression model is fit to the training data using LinearRegression from the sklearn.linear_model library. The predicted values of Y for the training and test sets are computed using predict method of the fitted model. The mean squared error is then calculated for the training and test sets using the mean_squared_error function from sklearn.metrics.

Similarly, a quadratic polynomial is fit to the training data using PolynomialFeatures from sklearn.preprocessing and LinearRegression from sklearn.linear_model. The predicted values of Y for the training and test sets are computed using predict method of the fitted model. The mean squared error is then calculated for the training and test sets using the mean_squared_error function.

Finally, a 19th degree polynomial is fit to the training data using PolynomialFeatures and LinearRegression. The predicted values of Y for the training and test sets are computed using predict method of the fitted model. The mean squared error is then calculated for the training and test sets using the mean_squared_error function.

The computed least square errors for each model are printed for both the training set and the test set.

```
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Part iii
# Split the data into training and test sets
X_train, Y_train = X[:20].reshape(-1, 1), Y[:20].reshape(-1, 1)
X_test, Y_test = X[20:].reshape(-1, 1), Y[20:].reshape(-1, 1)

# Fit a linear regression model
model_lr = LinearRegression()
model_lr.fit(X_train, Y_train)
Y_lr_train = model_lr.predict(X_train)
Y_lr_test = model_lr.predict(X_test)
err_lr_train = mean_squared_error(Y_train, Y_lr_train)
err_lr_test = mean_squared_error(Y_test, Y_lr_test)

# Fit a quadratic polynomial
poly = PolynomialFeatures(degree=2)
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.fit_transform(X_test)
model_quad = LinearRegression()
model_quad.fit(X_poly_train, Y_train)
Y_quad_train = model_quad.predict(X_poly_train)
Y_quad_test = model_quad.predict(X_poly_test)
err_quad_train = mean_squared_error(Y_train, Y_quad_train)
err_quad_test = mean_squared_error(Y_test, Y_quad_test)

# Fit a 19th degree polynomial
poly19 = PolynomialFeatures(degree=19)
X_poly19_train = poly19.fit_transform(X_train)
X_poly19_test = poly19.fit_transform(X_test)
model_poly19 = LinearRegression()
model_poly19.fit(X_poly19_train, Y_train)
Y_poly19_train = model_poly19.predict(X_poly19_train)
Y_poly19_test = model_poly19.predict(X_poly19_test)
err_poly19_train = mean_squared_error(Y_train, Y_poly19_train)
err_poly19_test = mean_squared_error(Y_test, Y_poly19_test)

# Print the errors
print("Least square errors on the training set:")
print(f"Linear regression: {err_lr_train}")
print(f"Quadratic polynomial: {err_quad_train}")
print(f"19th degree polynomial: {err_poly19_train}")

print("\nLeast square errors on the test set:")
print(f"Linear regression: {err_lr_test}")
print(f"Quadratic polynomial: {err_quad_test}")
print(f"19th degree polynomial: {err_poly19_test}")

```

## Part iv: Fitting the previous three models and computes the least square error for each model over a different training and test data set

In this section of the code, the objective is to repeat the analysis from part (iii) with a different split of the data into training and test sets. The new split uses the first 10 and last 10 data points as the training set, and the remaining 10 points as the test set. The models are then fitted to the training data, and their performance is evaluated on the test data.

To achieve this, the code starts by splitting the data into training and test sets using the numpy concatenate() function to combine the first 10 and last 10 points as the training set, and slicing the remaining 10 points as the test set.

After that, the code fits three different models to the training data: a linear model, a quadratic model, and a 19th degree polynomial model. For each model, the code computes the least squared error for both the training and test data. This is done by implementing the code from the previous step, and the resulting least square errors for each model are printed for both the training and test sets.

```
# Part iv
# Split the data into training and test sets
X_train = np.concatenate((X[:10], X[-10:]))
Y_train = np.concatenate((Y[:10], Y[-10:]))
X_test = X[10:20]
Y_test = Y[10:20]

# Fit a linear model to the training data
lin_reg = LinearRegression()
lin_reg.fit(X_train.reshape(-1, 1), Y_train)
lin_train_error = np.mean((lin_reg.predict(X_train.reshape(-1, 1)) - Y_train)**2)
lin_test_error = np.mean((lin_reg.predict(X_test.reshape(-1, 1)) - Y_test)**2)

# Fit a quadratic model to the training data
poly_reg = PolynomialFeatures(degree=2)
X_poly_train = poly_reg.fit_transform(X_train.reshape(-1, 1))
quad_reg = LinearRegression()
quad_reg.fit(X_poly_train, Y_train)
quad_train_error = np.mean((quad_reg.predict(X_poly_train) - Y_train)**2)
X_poly_test = poly_reg.transform(X_test.reshape(-1, 1))
quad_test_error = np.mean((quad_reg.predict(X_poly_test) - Y_test)**2)

# Fit a 19th degree polynomial to the training data
poly_reg = PolynomialFeatures(degree=19)
X_poly_train = poly_reg.fit_transform(X_train.reshape(-1, 1))
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_poly_train, Y_train)
poly_train_error = np.mean((poly_reg_model.predict(X_poly_train) - Y_train)**2)
X_poly_test = poly_reg.transform(X_test.reshape(-1, 1))
poly_test_error = np.mean((poly_reg_model.predict(X_poly_test) - Y_test)**2)


# Print the results
print("Least square errors on the training set:")
print(f"Linear model: {lin_train_error:.2f}")
print(f"Quadratic model: {quad_train_error:.2f}")
print(f"19th degree polynomial: {poly_train_error:.2f}")

print("\nLeast square errors on the test set:")
print(f"Linear model: {lin_test_error:.2f}")
print(f"Quadratic model: {quad_test_error:.2f}")
print(f"19th degree polynomial: {poly_test_error:.2f}")
```






## Computational Results

## Part i: Determine the parameters A, B, C, D for the minimum error 
After running the code, we obtained the following output:
```
Minimum error: 1.7394055581145358
A: -1.910167364651562
B: -0.7510349575309938
C: 0.7109310243744663
D: 31.816372221863755
```
  

## Part ii: 2D Loss Landscape 

We generated six 2D loss landscapes by fixing two parameters of the model f(x) = Acos(Bx) + Cx + D and sweeping through values of the other two parameters. These landscapes represent the variation of the error as a function of two model parameters. Each landscape showed different patterns that indicate the dependence or independence between the parameters.

The 2D loss landscape graph visualizes the complex interaction between different parameters and their impact on the loss function. It shows the variation in the loss function as we sweep through different combinations of two fixed parameters and two swept parameters, with darker shades indicating higher loss values and lighter shades indicating lower loss values. The minima in the loss landscape correspond to the optimal values of the parameters that result in the lowest possible loss. By observing the number of minima, we can understand the complexity of the optimization problem and select suitable optimization algorithms and initial parameter values to improve the convergence rate and avoid local minima.

![ABloss](https://user-images.githubusercontent.com/104536898/231589531-921ce11c-3c35-4005-a488-eeb3a6ae78e0.png)

![ADloss](https://user-images.githubusercontent.com/104536898/231590173-0c4c52c7-f8c7-49d5-8d8c-31daf313a02a.png)

In the A vs B landscape, we observed a dark vertical line at 0, indicating that A is independent of B. A is minimum at 0, while B is minimum at all values of B. In contrast, the A vs D landscape showed a dark circle in the middle of the graph, indicating that both A and D affect the error. The minimum error was observed when A is 0 and D is 30.

![CDloss](https://user-images.githubusercontent.com/104536898/231590673-9cfbf219-6af0-4bed-b1dd-1920fde139c6.png)

We observed a dark horizontal line at 30 in the C vs D landscape, indicating that D is independent of C. This occurs because D is minimum at 30, while C is minimum at all values of C.

![ACloss](https://user-images.githubusercontent.com/104536898/231590015-a96c1616-19b5-4710-9a85-14b169dd3f2e.png)

![BCloss](https://user-images.githubusercontent.com/104536898/231590376-dcf607d3-5a23-4b6d-b014-ef4c22418988.png)

![BDloss](https://user-images.githubusercontent.com/104536898/231590509-977ff209-22ee-4003-ab59-17672ff9bb62.png)

The A vs C and B vs C landscapes both displayed dark horizontal lines at 0, revealing that C is independent of A and B, respectively. In contrast, the B vs D landscape showed a dark horizontal line at 30, indicating that D is independent of B. In this case, D is minimum at 30, while B is minimum at all values of B.


## Part iii: Fitting three different models  to the data and computes the least square error for each model over training and test data
After running the code, we obtained the following output: 
```
Least square errors on the training set:
Linear regression: 5.029924812030078
Quadratic polynomial: 4.517917521075417
19th degree polynomial: 3.515402409224644

Least square errors on the test set:
Linear regression: 11.314065546641695
Quadratic polynomial: 75.92772737577081
19th degree polynomial: 750123339825593.6
```


## Part iv: Fitting the previous three models and computes the least square error for each model over a different training and test data set
After running the code, we obtained the following output:
```
Least square errors on the training set:
Linear model: 3.43
Quadratic model: 3.43
19th degree polynomial: 3.65

Least square errors on the test set:
Linear model: 8.65
Quadratic model: 8.44
19th degree polynomial: 25.42
```


## Summary and Conclusions 

## Part i: Determine the parameters A, B, C, D for the minimum error 
Using the parameters A, B, C, D from the results section, we can write the equation for f(x) as 

    f(x) = -1.91 cos(-0.751x) + 0.711 x + 31.816

This model gives us the best least-squares error of 1.739, indicating that it is a good fit for the given data. Therefore, we can conclude that the cosine function and linear term are important in predicting the values of Y for the given X values, and the model can be used for future predictions with reasonable accuracy.

## Part ii: 2D Loss Landscape 

The 2D loss landscapes generated by sweeping through values of two parameters of the model f(x) = Acos(Bx) + Cx + D provide a useful visual representation of the variation of the error as a function of the model parameters. The observed patterns in each landscape indicate the dependence or independence between the parameters. By examining the landscapes, we identified different combinations of the parameters that result in the minimum error.

Overall, the 2D loss landscapes provide valuable insights into the behavior of the model and can help us understand how changes in the model parameters affect the error. They can be used to explore the parameter space and identify the regions that correspond to the minimum error, which is crucial for model selection and parameter tuning. Therefore, the 2D loss landscapes are a powerful tool that can help researchers and practitioners in various fields, such as machine learning and data analysis, to improve the performance of their models and make better decisions.

## Part iii: Fitting three different models  to the data and computes the least square error for each model over training and test data

The results suggest that the 19th degree polynomial model has the lowest error on the training set, while the linear regression model has the highest. However, the 19th degree polynomial model has an extremely high error on the test set, suggesting overfitting. In contrast, the linear regression model performs better on the test set, indicating better generalization to new data. Therefore, we can conclude that the linear regression model is a better choice compared to the 19th degree polynomial model, as it has a lower test error and can provide better predictions for new data.

## Part iv: Fitting the previous three models and computes the least square error for each model over a different training and test data set

The results show that both the linear and quadratic models are good choices for fitting this dataset, as they have similar errors on both the training and test sets. However, the 19th degree polynomial model is not recommended as it has the highest training error and significantly higher test error, indicating overfitting. Among the three models, the quadratic model had the lowest error on both sets, suggesting it is the better model for this dataset.

Moreover, comparing the errors of this dataset to the previous one reveals that both training and test errors are smaller on this dataset, and the test error of the previous dataset's 19th degree polynomial model is significantly larger. The previous dataset used the first 20 data points as training data and the remaining 10 as test data, while the current dataset used the first 10 and last 10 data points as training data and the remaining 10 as test data.

In conclusion, this part of the analysis highlights the importance of choosing an appropriate model to avoid overfitting and emphasizes the value of comparing results to previous analyses to gain a better understanding of the dataset and achieve the lowest training and testing errors.
