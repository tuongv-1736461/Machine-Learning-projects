
# Comparative Analysis of Neural Network Architectures for Lorenz Equations Forecasting


## Author
- [Jenny Van](https://github.com/tuongv-1736461)

## Abstract

This report compares the performance of feed-forward neural networks, LSTM (Long Short-Term Memory) networks, RNN (Recurrent Neural Networks), and Echo State Networks for forecasting the dynamics of the Lorenz equations. The neural networks are trained to advance the solution from time t to t + Δt for various values of ρ (10, 28, and 40). The predictive capabilities of the networks are then evaluated for future state estimation using ρ values of 17 and 35. The study highlights the accuracy, convergence speed, and generalization abilities of each network architecture. The results contribute to understanding the suitability of different neural network models for forecasting complex systems like the Lorenz equations.

## Introduction 
Accurate prediction and forecasting of complex dynamical systems are vital in scientific and engineering fields. The Lorenz equations, which represent atmospheric convection, hold particular significance in understanding weather forecasting, climate modeling, and related applications.

This report focuses on investigating the Lorenz equations with two key objectives. Firstly, we train a neural network to advance the system's solution from time t to t + ∆t for different values of ρ (ρ = 10, 28, and 40). Subsequently, we assess the network's performance in predicting future states for ρ = 17 and ρ = 35, providing insights into its ability to capture the Lorenz system's dynamics beyond training data.

Additionally, we conduct a comprehensive comparison of four neural network architectures: feed-forward, LSTM, RNN, and Echo State Networks. These architectures have varying capabilities in capturing temporal dependencies and modeling complex nonlinear systems. Through this analysis, we aim to identify the strengths and weaknesses of each architecture in forecasting the dynamics of the Lorenz system.

## Theoretical Background

Modeling complex nonlinear systems with temporal dependencies is a challenging task that requires specialized neural network architectures. In this report, we compare four such architectures: feed-forward neural network, Long Short-Term Memory (LSTM), Recurrent Neural Network (RNN), and Echo State Networks (ESN). We investigate their capabilities in capturing temporal dependencies and modeling the dynamics of the Lorenz equations, a system known for its chaotic behavior.

Feed-forward Neural Network:
The feed-forward neural network is a foundational architecture widely used for various tasks, including pattern recognition and function approximation. It consists of an input layer, one or more hidden layers, and an output layer. However, feed-forward networks lack explicit memory of past inputs, making them less suitable for capturing temporal dependencies in sequential data.

Long Short-Term Memory (LSTM):
LSTM networks are designed to address the limitations of traditional RNNs in capturing long-term dependencies. They utilize memory cells and gating mechanisms to retain and update information over multiple time steps. LSTMs excel in modeling sequences where past information is crucial for accurate predictions. By incorporating memory cells, LSTMs can selectively retain relevant information and mitigate the vanishing or exploding gradient problem.

Recurrent Neural Network (RNN):
RNNs are recurrently connected networks that maintain hidden states, allowing them to capture temporal dependencies in sequential data. They have feedback connections that enable the network to retain information from previous time steps. RNNs are well-suited for modeling time series data and sequential patterns. However, they may suffer from the vanishing or exploding gradient problem, limiting their ability to capture long-term dependencies effectively.

Echo State Networks (ESN):
ESNs are a specialized type of recurrent neural network with randomly generated internal connections. Only the output weights are trained, while the internal states evolve naturally. ESNs have the unique property of "echo state," where the network's internal dynamics echo the input signal's properties. This allows ESNs to efficiently capture complex temporal dynamics while maintaining computational efficiency. ESNs are particularly effective in handling high-dimensional and noisy input data.

In this study, we employ these four architectures to model the Lorenz equations, which represent a nonlinear dynamical system with chaotic behavior. We compare their abilities to capture temporal dependencies and accurately model the system's dynamics. By training the networks to advance the solution from time t to t + ∆t for different parameter values of ρ, we assess their performance in predicting future states for the Lorenz system.


## Algorithm Implementation and Development

#### Generate training and testing data using Lorenz equation

This implementation generates multiple trajectories of the Lorenz equation for different values of rho using random initial conditions. The trajectories are then used to create input-output pairs for training and testing a model.

First, the necessary libraries are imported

```
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
from scipy import integrate
from mpl_toolkits.mplot3d import Axes3D
```
Next, the parameters are set up. dt represents the time step size, T is the total time duration, and t is an array of time values from 0 to T with a step size of dt. Additionally, beta, sigma, rho_train, and rho_test are parameters of the Lorenz equation. rho_train and rho_test contain different values of rho for generating training and testing data.
```
dt = 0.01
T = 8
t = np.arange(0, T+dt, dt)
beta = 8/3
sigma = 10
rho_train = [10, 28, 40]
rho_test = [17, 35]
```
Arrays are created to store the training and testing data. nn_input_train and nn_output_train are initialized as arrays of zeros to store the training input and output data. Similarly, nn_input_test and nn_output_test are initialized for testing data.
```
nn_input_train = np.zeros((100*(len(t)-1)*len(rho_train), 3))
nn_output_train = np.zeros_like(nn_input_train)
nn_input_test = np.zeros((100*(len(t)-1)*len(rho_test), 3))
nn_output_test = np.zeros_like(nn_input_test)
```
The derivative function lorenz_deriv is defined. It takes the state variables x_y_z, time t, and parameters sigma, beta, and rho. The function calculates the derivatives of the Lorenz equations and returns them as a list.
```
def lorenz_deriv(x_y_z, t, sigma=sigma, beta=beta, rho=rho_train[0]):
    x, y, z = x_y_z
    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]
```
The code proceeds to generate the training data. First, the random seed is set for reproducibility. Initial conditions for the training data, x0_train, are generated as random values between -15 and 15. The code then iterates over each value of rho in rho_train. For each rho, the Lorenz equations are solved using the integrate.odeint function. This function numerically integrates the lorenz_deriv function over the time array t, starting from each initial condition in x0_train. The resulting trajectories are stored in the x_t array. The code further iterates over each trajectory in x_t and populates the nn_input_train and nn_output_train arrays accordingly. Each trajectory contributes to the arrays by skipping the first point and using the previous point as input and the next point as output. The index k is updated to keep track of the position in the arrays.
```
# Generate training data
np.random.seed(123)
x0_train = -15 + 30 * np.random.random((100, 3))

k = 0
for rho in rho_train:
    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(rho,))
                      for x0_j in x0_train])
    for j in range(100):
        nn_input_train[k*(len(t)-1):(k+1)*(len(t)-1), :] = x_t[j, :-1, :]
        nn_output_train[k*(len(t)-1):(k+1)*(len(t)-1), :] = x_t[j, 1:, :]
        k += 1
```
To generate the testing data, a different random seed is set. The initial conditions x0_test for the testing data are generated in the same way as for the training data. The code iterates over each value of rho in rho_test, solving the Lorenz equations using a similar process as for the training data. The nn_input_test and nn_output_test arrays are populated by skipping the first point, using the previous point as input, and the next point as output. The index k is updated to track the position in the arrays.
```
# Generate testing data
np.random.seed(456)
x0_test = -15 + 30 * np.random.random((100, 3))

k = 0
for rho in rho_test:
    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(rho,))
                      for x0_j in x0_test])
    for j in range(100):
        nn_input_test[k*(len(t)-1):(k+1)*(len(t)-1), :] = x_t[j, :-1, :]
        nn_output_test[k*(len(t)-1):(k+1)*(len(t)-1), :] = x_t[j, 1:, :]
        k += 1
```
To visualize the training and testing data for different values of rho, 3D plots are generated. The training data is plotted first, with each plot representing a specific rho value. For each rho value, the code iterates through 100 trajectories and plots the corresponding trajectory using the x, y, and z values from nn_input_train. The starting points of each trajectory are marked with red dots. The process is repeated for the testing data, following the same steps as for the training data. These visualizations provide a graphical representation of the Lorenz system trajectories for different rho values in both the training and testing datasets.
```
rcParams.update({'font.size': 18})
plt.rcParams['figure.figsize'] = [12, 12]

# Plot the training data separately for each rho value
for j, rho_value in enumerate(rho_train):
    fig, ax = plt.subplots(1, 1, subplot_kw={'projection': '3d'})
    ax.set_title(f'Trajectories of Training Data Generated from the Lorenz Equation - ρ = {rho_value}')

    for k in range(100):
        index = j * 100 + k
        x, y, z = nn_input_train[index * (len(t) - 1):(index + 1) * (len(t) - 1), :].T
        ax.plot(x, y, z, linewidth=1)
        ax.scatter(x0_train[index % 100, 0], x0_train[index % 100, 1], x0_train[index % 100, 2], color='r')

    ax.view_init(18, -113)
    plt.show()

# Plot the testing data separately for each rho value
for j, rho_value in enumerate(rho_test):
    fig, ax = plt.subplots(1, 1, subplot_kw={'projection': '3d'})
    ax.set_title(f'Trajectories of Testing Data Generated from the Lorenz Equation - ρ = {rho_value}')

    for k in range(100):
        index = j * 100 + k
        x, y, z = nn_input_test[index * (len(t) - 1):(index + 1) * (len(t) - 1), :].T
        ax.plot(x, y, z, linewidth=1)
        ax.scatter(x0_test[index % 100, 0], x0_test[index % 100, 1], x0_test[index % 100, 2], color='r')

    ax.view_init(18, -113)
    plt.show()
```
#### Comparison of FNN, LSTM, RNN, and ESN models on Lorenz-generated data 
Multiple runs of training and evaluation for four different neural network models: FNN (Feed-forward Neural Network), LSTM (Long Short-Term Memory), RNN (Recurrent Neural Network), and ESN (Echo State Network) will be perform on Lorenz-generated data. The goal is to determine the best model among these four options.

The code begins by importing the required libraries and modules, including torch, torch.nn, and torch.optim, for working with PyTorch.

```
import torch
import torch.nn as nn
import torch.optim as optim
```
First, three activation functions are defined: logsig, radbas, and purelin. The logsig function applies the sigmoid activation, radbas calculates the radial basis function, and purelin simply returns the input value.

```
# Define activation functions
def logsig(x):
    return 1 / (1 + torch.exp(-x))

def radbas(x):
    return torch.exp(-torch.pow(x, 2))

def purelin(x):
    return x
```
Next, four models are defined: FNN, LSTMModel, RNNModel, and ESNModel. Each model inherits from the nn.Module class and implements a forward pass. The FNN model consists of three linear layers with 10 hidden units each, followed by the activation functions logsig, radbas, and purelin. The LSTMModel, RNNModel, and ESNModel use the corresponding modules provided by PyTorch and apply the purelin activation to the output.
```
# Define the feed-forward neural network (FNN) model
class FNN(nn.Module):
    def __init__(self):
        super(FNN, self).__init__()
        self.fc1 = nn.Linear(in_features=3, out_features=10)
        self.fc2 = nn.Linear(in_features=10, out_features=10)
        self.fc3 = nn.Linear(in_features=10, out_features=3)
        
    def forward(self, x):
        x = logsig(self.fc1(x))
        x = radbas(self.fc2(x))
        x = purelin(self.fc3(x))
        return x

# Define the LSTM model
class LSTMModel(nn.Module):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=3, hidden_size=10, num_layers=1, batch_first=True)
        self.fc = nn.Linear(in_features=10, out_features=3)
        
    def forward(self, x):
        x, _ = self.lstm(x)
        x = purelin(self.fc(x[:, -1, :]))
        return x

# Define the RNN model
class RNNModel(nn.Module):
    def __init__(self):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_size=3, hidden_size=10, num_layers=1, batch_first=True)
        self.fc = nn.Linear(in_features=10, out_features=3)
        
    def forward(self, x):
        x, _ = self.rnn(x)
        x = purelin(self.fc(x[:, -1, :]))
        return x

# Define the Echo State Network (ESN) model
class ESNModel(nn.Module):
    def __init__(self):
        super(ESNModel, self).__init__()
        self.esn = nn.RNN(input_size=3, hidden_size=10, num_layers=1, batch_first=True)
        self.fc = nn.Linear(in_features=10, out_features=3)
        
    def forward(self, x):
        x, _ = self.esn(x)
        x = purelin(self.fc(x[:, -1, :]))
        return x
```
Instances of each model are created: fnn_model, lstm_model, rnn_model, and esn_model.
```
# Create instances of the models
fnn_model = FNN()
lstm_model = LSTMModel()
rnn_model = RNNModel()
esn_model = ESNModel()
```
Loss functions and optimizers are defined for each model. The mean squared error (MSE) loss is used as the criterion, and the stochastic gradient descent (SGD) optimizer with a learning rate of 0.01 and momentum of 0.9 is instantiated for each model.
```
# Define loss functions and optimizers for each model
criterion = nn.MSELoss()
fnn_optimizer = optim.SGD(fnn_model.parameters(), lr=0.01, momentum=0.9)
lstm_optimizer = optim.SGD(lstm_model.parameters(), lr=0.01, momentum=0.9)
rnn_optimizer = optim.SGD(rnn_model.parameters(), lr=0.01, momentum=0.9)
esn_optimizer = optim.SGD(esn_model.parameters(), lr=0.01, momentum=0.9)
```
Using 10 runs in the training and evaluation process offers several advantages. First, it enhances the robustness of the results by accounting for randomness and variability in the models. Multiple runs help observe the models' performance across different initializations, data shuffling, and mini-batch selections. This ensures a more reliable assessment of the models' capabilities and reduces the impact of random factors. Second, using the final loss of each run to calculate the average loss and variability provides a more comprehensive understanding of the models' performance. It captures the models' overall performance after training for a specific number of epochs, giving a better indication of convergence. Additionally, calculating the variability of the losses helps assess the consistency of the models' performance across the runs, indicating how stable the results are.

First, the number of runs is set to 10, and arrays and lists are defined to store the average testing losses and final losses for each run of the models.
```
# Set the number of runs
num_runs = 10

# Define arrays to store the average testing losses for each run
fnn_avg_test_losses = []
lstm_avg_test_losses = []
rnn_avg_test_losses = []
esn_avg_test_losses = []

# Define lists to store the final loss for each run
fnn_final_losses = []
lstm_final_losses = []
rnn_final_losses = []
esn_final_losses = []
```
The code then enters a loop that iterates over each run. Within each run, the models and optimizers are reset. The models include FNN, LSTM, RNN, and ESN models, and the optimizers are initialized with specific learning rates and momentum values.
```
# Perform multiple runs
for run in range(num_runs):
    print(f"Run {run+1}")

    # Reset the models and optimizers for each run
    fnn_model = FNN()
    lstm_model = LSTMModel()
    rnn_model = RNNModel()
    esn_model = ESNModel()

    fnn_optimizer = optim.SGD(fnn_model.parameters(), lr=0.01, momentum=0.9)
    lstm_optimizer = optim.SGD(lstm_model.parameters(), lr=0.01, momentum=0.9)
    rnn_optimizer = optim.SGD(rnn_model.parameters(), lr=0.01, momentum=0.9)
    esn_optimizer = optim.SGD(esn_model.parameters(), lr=0.01, momentum=0.9)
```
Next, lists are defined to store the losses for each epoch of training. The models are trained for 30 epochs using a loop. In each epoch, the models are trained on the training data by performing forward propagation, calculating the loss using a specified criterion, performing backward propagation to compute gradients, and updating the model parameters using the optimizers.
```
    # Define lists to store the losses for each epoch
    fnn_epoch_losses = []
    lstm_epoch_losses = []
    rnn_epoch_losses = []
    esn_epoch_losses = []

    # Train the models
    for epoch in range(30):
        # Train the feed-forward neural network (FNN)
        fnn_optimizer.zero_grad()
        fnn_outputs = fnn_model(nn_input_train)
        fnn_loss = criterion(fnn_outputs, nn_output_train)
        fnn_loss.backward()
        fnn_optimizer.step()

        # Train the LSTM model
        lstm_optimizer.zero_grad()
        lstm_outputs = lstm_model(nn_input_train.unsqueeze(1))
        lstm_loss = criterion(lstm_outputs, nn_output_train)
        lstm_loss.backward()
        lstm_optimizer.step()

        # Train the RNN model
        rnn_optimizer.zero_grad()
        rnn_outputs = rnn_model(nn_input_train.unsqueeze(1))
        rnn_loss = criterion(rnn_outputs, nn_output_train)
        rnn_loss.backward()
        rnn_optimizer.step()

        # Train the Echo State Network (ESN) model
        esn_optimizer.zero_grad()
        esn_outputs = esn_model(nn_input_train.unsqueeze(1))
        esn_loss = criterion(esn_outputs, nn_output_train)
        esn_loss.backward()
        esn_optimizer.step()
```
The losses for each epoch are stored in the respective lists for each model. After training, the final loss for each run is appended to the corresponding final loss lists.
```
        # Store the losses for each epoch
        fnn_epoch_losses.append(fnn_loss.item())
        lstm_epoch_losses.append(lstm_loss.item())
        rnn_epoch_losses.append(rnn_loss.item())
        esn_epoch_losses.append(esn_loss.item())

    # Store the final loss for each run
    fnn_final_losses.append(fnn_loss.item())
    lstm_final_losses.append(lstm_loss.item())
    rnn_final_losses.append(rnn_loss.item())
    esn_final_losses.append(esn_loss.item())
```
A plot is generated to visualize the loss performance for each run, showing the epoch losses for FNN, LSTM, RNN, and ESN models.
```
    # Plot the loss performance for each run
    epochs = range(1, 31)
    plt.figure(figsize=(8, 6))
    plt.plot(epochs, fnn_epoch_losses, label="FNN")
    plt.plot(epochs, lstm_epoch_losses, label="LSTM")
    plt.plot(epochs, rnn_epoch_losses, label="RNN")
    plt.plot(epochs, esn_epoch_losses, label="ESN")

    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'Loss Performance - Run {run+1}')
    plt.legend()
```
Then, the testing losses for each model are calculated using the trained models and the testing data. These testing losses are stored in separate lists for each run.
```
    # Calculate the testing losses for each model
    fnn_test_outputs = fnn_model(nn_input_test)
    fnn_test_loss = criterion(fnn_test_outputs, nn_output_test)

    lstm_test_outputs = lstm_model(nn_input_test.unsqueeze(1))
    lstm_test_loss = criterion(lstm_test_outputs, nn_output_test)

    rnn_test_outputs = rnn_model(nn_input_test.unsqueeze(1))
    rnn_test_loss = criterion(rnn_test_outputs, nn_output_test)

    esn_test_outputs = esn_model(nn_input_test.unsqueeze(1))
    esn_test_loss = criterion(esn_test_outputs, nn_output_test)

    # Store the testing losses for each run
    fnn_avg_test_losses.append(fnn_test_loss.item())
    lstm_avg_test_losses.append(lstm_test_loss.item())
    rnn_avg_test_losses.append(rnn_test_loss.item())
    esn_avg_test_losses.append(esn_test_loss.item())

plt.show()
```
After completing all the runs, the average and variability of the training losses for FNN, LSTM, RNN, and ESN models are calculated using the np.mean() and np.var() functions from NumPy. The average and variability of the losses are printed for each model. The average testing losses for each model are also calculated by averaging the testing losses across all runs and printed.
```
# Calculate the average and variability of the losses using the final loss for each run
fnn_avg_loss = np.mean(fnn_final_losses)
fnn_var_loss = np.var(fnn_final_losses)
lstm_avg_loss = np.mean(lstm_final_losses)
lstm_var_loss = np.var(lstm_final_losses)
rnn_avg_loss = np.mean(rnn_final_losses)
rnn_var_loss = np.var(rnn_final_losses)
esn_avg_loss = np.mean(esn_final_losses)
esn_var_loss = np.var(esn_final_losses)

# Print the average and variability of the losses with a maximum of 6 decimal places
print("FNN - Average Loss: {:.6f}".format(fnn_avg_loss))
print("LSTM - Average Loss: {:.6f}".format(lstm_avg_loss))
print("RNN - Average Loss: {:.6f}".format(rnn_avg_loss))
print("ESN - Average Loss: {:.6f}".format(esn_avg_loss))
print("FNN - Variability of Loss: {:.6f}".format(fnn_var_loss))
print("LSTM - Variability of Loss: {:.6f}".format(lstm_var_loss))
print("RNN - Variability of Loss: {:.6f}".format(rnn_var_loss))
print("ESN - Variability of Loss: {:.6f}".format(esn_var_loss))

# Calculate the average testing losses for each model
fnn_avg_test_loss = sum(fnn_avg_test_losses) / num_runs
lstm_avg_test_loss = sum(lstm_avg_test_losses) / num_runs
rnn_avg_test_loss = sum(rnn_avg_test_losses) / num_runs
esn_avg_test_loss = sum(esn_avg_test_losses) / num_runs

# Print the average testing losses for each model
print("Average Testing Losses:")
print("FNN:", fnn_avg_test_loss)
print("LSTM:", lstm_avg_test_loss)
print("RNN:", rnn_avg_test_loss)
print("ESN:", esn_avg_test_loss)
```

## Result

#### Generate training and testing data using Lorenz equation
![p10](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/p=10.png)
![p28](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/p=28.png)
![p40](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/p=40.png)
![p17](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/p=17.png)
![p35](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/p=35.png)

#### Comparison of FNN, LSTM, RNN, and ESN models on Lorenz-generated data 

When comparing the FNN, LSTM, RNN, and ESN models on the Lorenz-generated data based on the average testing loss and training loss from 10 runs, we observe the following results:
![run1](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run1.png)
![run2](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run2.png)
![run3](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run3.png)
![run4](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run4.png)
![run5](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run5.png)
![run6](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run6.png)
![run7](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run7.png)
![run8](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run8.png)
![run9](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run9.png)
![run10](https://github.com/tuongv-1736461/EE399/blob/main/EE399HW5/run10.png)
| Model | Average Training Loss | Variability of Training Loss | Average Testing Loss |
|-------|----------------------|---------------------|----------------------|
| FNN   | 8.7655               | 0.0000                | 7.5274               |
| LSTM  | 4.1878               | 0.0000                | 4.4978               |
| RNN   | 2.5301               | 0.0000                | 2.8251               |
| ESN   | 2.2587               | 0.0000                | 3.1100               |

The FNN model has the highest average training loss and average testing loss among the four models. This can be attributed to the limitations of the FNN architecture in capturing the complex dynamics and temporal dependencies present in the Lorenz-generated data. The FNN model lacks the recurrent connections necessary for effectively modeling sequential information, which leads to higher losses.

The LSTM model performs better than the FNN model, demonstrating lower average training loss and average testing loss. LSTMs are specifically designed to address the issue of capturing long-term dependencies in sequential data. The LSTM's ability to retain information over long time steps enables it to better capture the dynamics of the Lorenz-generated data, resulting in improved performance compared to the FNN model.

The RNN model exhibits the lowest average training loss and average testing loss among the four models. RNNs are well-suited for modeling sequential data due to their recurrent connections that allow information to flow across different time steps. The RNN's ability to capture temporal dependencies effectively enables it to better understand and predict the dynamics of the Lorenz-generated data, resulting in the lowest losses.

The ESN model achieves a relatively low average training loss but a higher average testing loss compared to the RNN model. ESNs are a type of recurrent neural network that leverage the concept of reservoir computing. While ESNs can be powerful for certain tasks, the slightly higher average testing loss suggests that the ESN model may not have been as effective in capturing the complex dynamics of the Lorenz-generated data compared to the RNN model.

In summary, the FNN model performs the poorest in terms of both average training loss and average testing loss due to its inability to capture sequential dependencies. The LSTM model demonstrates improvement by incorporating memory cells to capture long-term dependencies. The RNN model, with its recurrent connections, outperforms both the FNN and LSTM models, achieving the lowest losses. The ESN model, while effective to some extent, falls short compared to the RNN model in capturing the dynamics of the Lorenz-generated data.


## Conclusion

In conclusion, the study compared different neural network (NN) models for advancing the Lorenz equations and predicting future states. The FNN model performed poorly due to its inability to capture sequential dependencies. The LSTM model showed improvement by incorporating memory cells, achieving lower losses. The RNN model outperformed both FNN and LSTM models, demonstrating the best performance by effectively capturing temporal dependencies. The ESN model, although somewhat effective, fell short compared to the RNN model. Recurrent connections proved crucial for modeling sequential data like the Lorenz equations. The findings emphasize the importance of choosing the appropriate NN architecture for specific tasks, with the RNN model being the preferred choice for forecasting Lorenz dynamics. Further experimentation and tuning are recommended for optimal model selection.
